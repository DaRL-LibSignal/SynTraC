{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare For Count Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "detection=False\n",
    "data_dir = f'./Dataset/detection_{detection}'\n",
    "traj_list = []\n",
    "if Path.exists(Path.joinpath(Path(data_dir), Path('dataset.npz'))):\n",
    "\tdata = np.load(str(Path.joinpath(Path(data_dir), Path('dataset.npz'))))\n",
    "\tfeatures, cnts, phases, rewards, actions, timeouts, terminals =data['features'], data['cnts'], data['phases'], data['rewards'], data['actions'], data['timeouts'], data['terminals']\n",
    "else:\n",
    "\tvision_extractor = PreTrainedModifiedResNet().to(\"cuda\")\n",
    "\tif Path.exists(Path(data_dir)):\n",
    "\t\ttraj_list = [traj for traj in os.listdir(data_dir) if 'traj' in traj]\n",
    "\telse:\n",
    "\t\tprocessed = process_meta_data('./Dataset/Intersection_camera.json')\n",
    "\t\tprint(\"Done meta data\")\n",
    "\t\tprocess_code = process_raw_data(processed, detection=detection)\n",
    "\t\ttraj_list = [traj for traj in os.listdir(data_dir) if 'traj' in traj]\n",
    "\tfeatures_list = []\n",
    "\tcnts_list = []\n",
    "\tphases_list = []\n",
    "\trewards_list = []\n",
    "\tactions_list = []\n",
    "\ttimeouts_list = []\n",
    "\tterminals_list = []\n",
    "\tprint(f'Total trajectories: {len(traj_list)}')\n",
    "\tfor traj in traj_list:\n",
    "\t\tdata = np.load(str(Path.joinpath(Path(data_dir), Path(traj))))\n",
    "\t\timgs, cnts, phases, rewards, actions, timeouts, terminals = data['imgs'], data['cnts'], data['phases'], data['rewards'], data['actions'], data['timeouts'], data['terminals']\n",
    "\t\t# img is too big to process\n",
    "\t\traw_states = imgs.reshape((-1, 3, 1080, 1920))\n",
    "\t\traw_data = torch.Tensor(raw_states)\n",
    "\t\traw_dataset = TensorDataset(raw_data)\n",
    "\t\traw_loader = DataLoader(raw_dataset, batch_size=16)\n",
    "\t\t# use pre-trained model to extract features\n",
    "\t\textracted_list = []\n",
    "\t\tpbar = tqdm(total=len(raw_loader))\n",
    "\t\tfor b in raw_loader:\n",
    "\t\t\textracted = vision_extractor(b[0].to(\"cuda\"))\n",
    "\t\t\textracted_list.append(extracted) \n",
    "\t\t\tpbar.update(1)\n",
    "\t\tfeatures = torch.concat(extracted_list, dim=0)\n",
    "\t\tfeatures = features.reshape(-1, 4, 2048)\n",
    "\t\tfeatures = features.numpy()\n",
    "\t\tfeatures_list.append(features)\n",
    "\t\tcnts_list.append(cnts)\n",
    "\t\tphases_list.append(phases)\n",
    "\t\trewards_list.append(rewards)\n",
    "\t\tactions_list.append(actions)\n",
    "\t\ttimeouts_list.append(timeouts)\n",
    "\t\tterminals_list.append(terminals)\n",
    "\tprint(features_list)\n",
    "\tfeatures = np.concatenate(features_list, axis=0)\n",
    "\tcnts =  np.concatenate(cnts_list, axis=0)\n",
    "\tphases = np.concatenate(phases_list, axis=0)\n",
    "\trewards = np.concatenate(rewards_list, axis=0) \n",
    "\tactions =  np.concatenate(actions_list, axis=0)\n",
    "\ttimeouts =  np.concatenate(timeouts_list, axis=0)\n",
    "\tterminals =  np.concatenate(terminals_list, axis=0)\n",
    "\tnp.savez(data_dir+ '/dataset.npz', features=features, cnts=cnts, phases=phases,\\\n",
    "\t\trewards=rewards, actions=actions, timeouts=timeouts, terminals=terminals)\n",
    "\n",
    "new_actions = []\n",
    "for i, act in enumerate(actions):\n",
    "\tif act == phases[i]:\n",
    "\t\tnew_actions.append(0)     \n",
    "\n",
    "\telse:\n",
    "\t\tnew_actions.append(1)\n",
    "actions = np.array(new_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Re-build dataset with cnt and phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 20:14.01 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('float32')], shape=[(9,)]) reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)])\n",
      "2024-04-05 20:14.01 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2024-04-05 20:14.01 [info     ] Action size has been automatically determined. action_size=4\n"
     ]
    }
   ],
   "source": [
    "from utils.transform import one_hot\n",
    "\n",
    "onehot_phases = one_hot(phases, 4)\n",
    "states = np.concatenate((cnts, phases.reshape(-1,1)-1), axis=1, dtype=np.float32)\n",
    "dataset = d3rlpy.dataset.MDPDataset(\n",
    "    observations=states,\n",
    "    actions=actions-1,\n",
    "    rewards=-rewards,\n",
    "    terminals=terminals,\n",
    "    timeouts=timeouts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Train a cnt based offline RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import  DQN, DQNConfig\n",
    "from d3rlpy.metrics.evaluators import TDErrorEvaluator\n",
    "from d3rlpy.metrics.evaluators import AverageValueEstimationEvaluator\n",
    "from d3rlpy.preprocessing import StandardObservationScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 16:52.35 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(9,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=4)\n",
      "2024-03-27 16:52.35 [info     ] Directory is created at d3rlpy_logs\\DiscreteCQL_20240327165235\n",
      "2024-03-27 16:52.35 [debug    ] Building models...            \n",
      "2024-03-27 16:52.35 [debug    ] Models have been built.       \n",
      "2024-03-27 16:52.35 [info     ] Parameters                     params={'observation_shape': [9], 'action_size': 4, 'config': {'type': 'discrete_cql', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'clip', 'params': {'low': -1.0, 'high': 1.0, 'multiplier': 1.0}}, 'learning_rate': 5e-05, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 0.0003125, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'qr', 'params': {'share_encoder': False, 'n_quantiles': 200}}, 'n_critics': 1, 'target_update_interval': 2000, 'alpha': 4.0}}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1618b13d6ff4700896692c40893903c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 16:53.08 [info     ] DiscreteCQL_20240327165235: epoch=1 step=5000 epoch=1 metrics={'time_sample_batch': 0.0008152896881103515, 'time_algorithm_update': 0.005668901443481446, 'loss': 8.324113044834137, 'td_loss': 3.4253884801864625, 'conservative_loss': 1.2246811416983605, 'time_step': 0.006611674165725708, 'value_scale': -1.9055350453160746} step=5000\n",
      "2024-03-27 16:53.08 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteCQL_20240327165235\\model_5000.d3\n"
     ]
    }
   ],
   "source": [
    "cql = d3rlpy.algos.DiscreteCQLConfig(\n",
    "    learning_rate=5e-5,\n",
    "    optim_factory=d3rlpy.models.optimizers.AdamFactory(eps=1e-2 / 32),\n",
    "    batch_size=32,\n",
    "    alpha=4.0,\n",
    "    q_func_factory=d3rlpy.models.q_functions.QRQFunctionFactory(\n",
    "        n_quantiles=200\n",
    "    ),\n",
    "    # observation_scaler=\n",
    "    target_update_interval=2000,\n",
    "    reward_scaler=d3rlpy.preprocessing.ClipRewardScaler(-1.0, 1.0),\n",
    ").create(device=\"cuda:0\")\n",
    "\n",
    "# env_scorer = d3rlpy.metrics.EnvironmentEvaluator(env, epsilon=0.001)\n",
    " \n",
    "cql.fit(\n",
    "    dataset,\n",
    "    n_steps=5000,\n",
    "\tn_steps_per_epoch=5000,\n",
    "        evaluators={\n",
    "            # 'td_error': TDErrorEvaluator(),\n",
    "            'value_scale': AverageValueEstimationEvaluator()\n",
    "        },\n",
    "    show_progress =True)\n",
    "cql.save_policy(\"cnt_cql_policy3_action_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 17:31.26 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(9,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=4)\n",
      "2024-03-27 17:31.26 [info     ] Directory is created at d3rlpy_logs\\DQN_20240327173126\n",
      "2024-03-27 17:31.26 [debug    ] Building models...            \n",
      "2024-03-27 17:31.26 [debug    ] Models have been built.       \n",
      "2024-03-27 17:31.26 [info     ] Parameters                     params={'observation_shape': [9], 'action_size': 4, 'config': {'type': 'dqn', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'clip', 'params': {'low': -1.0, 'high': 1.0, 'multiplier': 1.0}}, 'learning_rate': 5e-05, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 0.0003125, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 2000}}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139f5fe2a2874e0b890b6c64c61691e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 17:36.16 [info     ] DQN_20240327173126: epoch=1 step=50000 epoch=1 metrics={'time_sample_batch': 0.0011114589023590088, 'time_algorithm_update': 0.004289542679786682, 'loss': 0.18937634423396085, 'time_step': 0.005757480177879333, 'value_scale': -2.7958215391057983} step=50000\n",
      "2024-03-27 17:36.16 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20240327173126\\model_50000.d3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn = DQNConfig(\n",
    "    learning_rate=5e-5,\n",
    "    optim_factory=d3rlpy.models.optimizers.AdamFactory(eps=1e-2 / 32),\n",
    "    target_update_interval=2000,\n",
    "    reward_scaler=d3rlpy.preprocessing.ClipRewardScaler(-1.0, 1.0),\n",
    "    # observation_scaler=StandardObservationScaler()\n",
    ").create(device=\"cuda:0\")\n",
    "\n",
    "dqn.fit(dataset,\n",
    "        # eval_episodes=test_episodes,\n",
    "    n_steps=50000,\n",
    "\tn_steps_per_epoch=50000,\n",
    "        evaluators={\n",
    "            # 'td_error': TDErrorEvaluator(),\n",
    "            'value_scale': AverageValueEstimationEvaluator()\n",
    "        })\n",
    "dqn.save_policy(\"cnt_policy1_dqn_action_final_50000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:00.03 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(9,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=4)\n",
      "2024-03-27 20:00.03 [info     ] Directory is created at d3rlpy_logs\\DoubleDQN_20240327200003\n",
      "2024-03-27 20:00.03 [debug    ] Building models...            \n",
      "2024-03-27 20:00.04 [debug    ] Models have been built.       \n",
      "2024-03-27 20:00.04 [info     ] Parameters                     params={'observation_shape': [9], 'action_size': 4, 'config': {'type': 'double_dqn', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'learning_rate': 6.25e-05, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000}}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df388dc942a1498fb8767f5889b82988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:03.35 [info     ] DoubleDQN_20240327200003: epoch=1 step=50000 epoch=1 metrics={'time_sample_batch': 0.0007376984310150147, 'time_algorithm_update': 0.0032367820024490354, 'loss': 0.040612651078351776, 'time_step': 0.004196608924865722, 'value_scale': -3.2678763409557123} step=50000\n",
      "2024-03-27 20:03.35 [info     ] Model parameters are saved to d3rlpy_logs\\DoubleDQN_20240327200003\\model_50000.d3\n"
     ]
    }
   ],
   "source": [
    "from d3rlpy.algos import DoubleDQNConfig\n",
    "\n",
    "# Create Double DQN configuration\n",
    "double_dqn = DoubleDQNConfig().create(device=\"cuda:0\")\n",
    "\n",
    "# Fit the model\n",
    "double_dqn.fit(\n",
    "    dataset,\n",
    "    n_steps=50000,\n",
    "    n_steps_per_epoch=50000,\n",
    "    evaluators={\n",
    "        'value_scale': AverageValueEstimationEvaluator()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save the trained policy\n",
    "double_dqn.save_policy(\"cnt_policy1_double_dqn_action_final_50000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:22.07 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(9,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=4)\n",
      "2024-03-27 20:22.07 [info     ] Directory is created at d3rlpy_logs\\DiscreteSAC_20240327202207\n",
      "2024-03-27 20:22.07 [debug    ] Building models...            \n",
      "2024-03-27 20:22.07 [debug    ] Models have been built.       \n",
      "2024-03-27 20:22.07 [info     ] Parameters                     params={'observation_shape': [9], 'action_size': 4, 'config': {'type': 'discrete_sac', 'params': {'batch_size': 64, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'actor_learning_rate': 0.0003, 'critic_learning_rate': 0.0003, 'temp_learning_rate': 0.0003, 'actor_optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'critic_optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'temp_optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'actor_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'critic_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 2, 'initial_temperature': 1.0, 'target_update_interval': 8000}}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5f6ea84d01486c93c36e8b8ae8dd48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:27.13 [info     ] DiscreteSAC_20240327202207: epoch=1 step=30000 epoch=1 metrics={'time_sample_batch': 0.0012546623547871906, 'time_algorithm_update': 0.008601936809221904, 'temp_loss': -0.0013539146754672402, 'temp': 2.1042203056494393, 'critic_loss': 0.012549203067651251, 'actor_loss': -6.126500767902534, 'time_step': 0.010161516888936361, 'value_scale': 8.96070490943061} step=30000\n",
      "2024-03-27 20:27.13 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteSAC_20240327202207\\model_30000.d3\n"
     ]
    }
   ],
   "source": [
    "from d3rlpy.algos import DiscreteSACConfig\n",
    "\n",
    "# Create SAC configuration\n",
    "sac = DiscreteSACConfig().create(device=\"cuda:0\")\n",
    "\n",
    "# Fit the model\n",
    "sac.fit(\n",
    "    dataset,\n",
    "    n_steps=30000,\n",
    "    n_steps_per_epoch=30000,\n",
    "    evaluators={\n",
    "        'value_scale': AverageValueEstimationEvaluator()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save the trained policy\n",
    "sac.save_policy(\"cnt_policy1_sac_action_final_30000.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
